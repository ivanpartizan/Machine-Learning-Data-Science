AI or artificial intelligence simply means a human intelligence exhibited by machines. 
AI is a machine that acts like a human. 
And currently in our industry we have something called narrow AI, that is machines can be just as good or even better than humans at specific tasks.

But each AI is only good at one task. 
Narrow AI that we currently have simply means those machines can only do one thing really well.
They can't be like humans and have multiple abilities.
That is called general AI and it's something that we are very far away from now.

Machine Learning is a subset of AI and Machine Learning is an approach to try and achieve artificial intelligence through systems that can find patterns in a set of data.

Deep learning or deep neural networks is just one of the techniques for implementing Machine Learning.
For now, we can just think of it as a type of algorithm.

Data Science simply means analyzing data, looking at data and then doing something with it, usually some sort of a business goal.
So, when we talk about Machine Learning, there is a lot of overlap with Data Science.

The only reason that we care about Machine Learning is that we are able to use machines to predict results based on incoming data.

Machine Learning is using an algorithm or computer program to learn about different patterns in data and then taking that algorithm and what it learned to make predictions about the future using similar data.
ML algorithms are also called models.
Machine Learning algorithm looks at the input and then it looks at the output and it tries to figure out the set of instructions in between these two.
Machine models find patterns collected in data so we can use those patterns for future problems.

Data analysis is looking at a set of data and gaining an understanding of it by comparing different examples, different features and making visualizations like graphs. 
Data Science is running experiments on a set of data with the hopes of finding actionable insights within it.
One of these experiments may be to build a machine learning model.

We can consider Data Analysis and Machine Learning as a part of Data Science.

MACHINE LEARNING
lets computers make decisions about data 
and lets computers learn from data 
and then make predictions and decisions...

MACHINE LEARNING
is a general term for when computers learn from data, 
it allows computers to do tasks that in the past required humans. 

Machine learning comes in three parts -data collection, data modeling and deployment
We're going to cover data modeling, which means you'll be able to take a data set and apply machine
learning algorithms to find insights on that data

ML FRAMEWORK-six steps
1. Problem definition
What problem are we trying to solve? Supervised, unsupervised, classification, regression?
2. Data
What kind of data do we have?
Structured (rows and columns) or unstructured (like images or audio)?
3. Evaluation
What defines success for us?
4. Features
What features should we model? What do we already know about the data?
A machine learning algorithm's goal is to turn these features into patterns to make predictions.
5. Modelling
Based on our problem and data, what kind of ML model should we use?
6. Experimentation
How could we improve/what can we try next?

When shouldn't you use machine learning?
Will a simple hand-coded instruction based system work?

Main types of ML Problems:
SUPERVISED LEARNING
UNSUPERVISED LEARNING
TRANSFER LEARNING
REINFORCEMENT LEARNING

SUPERVISED LEARNING - you have data and labels
a ML algorithm tries to use the data to predict a label
if it guesses the label wrong, the algorithm corrects itself and tries again
supervised learning algorithm repeats this process over and over again trying to get better

Main types of supervised learning:
Classification and regression

Classification: 
involves predicting if something is one thing or another
If there are only two options, it is called binary classification
If there are more than two options, it is called multi-class classification

Regression:
involves problems trying to predict the number
"how much will this house sell for?"
"how many people will buy this app?"

UNSUPERVISED LEARNING - has data but no labels
you find patterns in the data and group customers (who purchase similar things together)
you provided the labels, they were not there to begin with but the patterns were
clusters - putting groups of similar examples together

TRANSFER LEARNING - leverages what one machine learning model has learned in another
machine learning model

REINFORCEMENT LEARNING - involves having a computer program performs some actions 
within a defined space and rewarding it for doing it well
or punishing it for doing poorly
example: teaching a ML algorithm to play chess

SUPERVISED LEARNING - 'I know my inputs and outputs'
UNSUPERVISED LEARNING - 'I am not sure of the outputs but I have inputs'
TRANSFER LEARNING - 'I think my problem may be similar to something else'

DATA
Different types of data:
STRUCTURED and UNSTRUCTURED

STRUCTURED data: Excel file, rows and columns
all of the samples are typically in similar format
UNSTRUCTURED data: images, natural text, videos and audio files

STATIC and STREAMING DATA
STATIC is data which doesn't change over time (CSV files)
STREAMING is data which is constantly changed over time

'The more data the better'

A data science workflow:
opening the CSV file in Jupyter notebook, a tool for building ML projects
exploring the data and performing data analysis using pandas, a Python library for data analysis
making visualisations such as graphs and comparing different data points using matplotlib, 
then building ML models on the data using scikit learn

EVALUATION
Every ML problem you come across will have the similar goal of finding insights in data 
to predict the future in some way
An evaluation metric is measure of how well ML algorithm predicts the future

Different types of metrics:
- for classification, or predicting whether some thing is one thing or another - accuracy, precision, recall
- for regression, or predicting a number (example, how much a car will sell for) - Mean absolute error 
or MAE, Mean squared error or MSE, root mean squared error of RMSE
- for recommendation problems: precision at K

FEATURES
Features is another word for different forms of data (different forms of data within structured or 
unstructured data)
Different features of data, example: weight, sex, heart rate, chest pain
Features could also be referred to as feature variables
We want to use feature variables to predict the target variable (example: heart disease?)

There are different kinds of feature variables:
numerical features (a number) and categorical features (one thing or another, like sex)
And then there is derived feature, which is when someone looks at the data and 
creates a new feature using the existing ones
It is often referred to as feature engineering - (process of) looking at different features of data and 
creating new ones/altering existing ones

A feature works best within ML algorithm if many of the samples have it

Feature coverage - how many samples have different features? Ideally, every sample has the same features. 
The process of ensuring all samples have similar information is called feature coverage.

MODELLING
3 SETS
Based on our problem and data, what ML model should we use?

Modelling can be broken down into three parts:
1. Choosing and training a model
2. Tuning a model
3. Model comparison

KEY - The most important concept in machine learning:
(the training, validation and test sets or 3 sets)
Split your data into 3 different sets:
1. Training set (train your model on this)
2. Validation set (tune your model on this)
3. Test set (test and compare your different models on this)

Example, exam: course materials (training set) -> practice exam (validation set) -> final exam (test set)
Generalization - the ability for a machine learning model to perform well on data it hasn't seen before

training split 70-80%
validation split 10-15%
test split 10-15%
All three of these sets are separate! During training, a model never sees a validation split or a test split
And during testing, you are doing it on a test split, not a training set

Modelling - Picking the model
1. Choosing and training a model - training data
2. Tuning a model - validation data
3. Model comparison - test data

Choosing a model:
There are many pre built ML models
Main goal would be knowing what kind of ML algorithm to use with what kind of problem

If you are working with structured data, decision trees such as Random Forest and 
gradient boosting algorithms like CatBoost and XGBoost tend to work best
If you are working with unstructured data, Deep Learning, neural networks and 
Transfer Learning tend to work best

after choosing a model, next step is to train it

Training a model - main goal will be to line up inputs and outputs
we want our model to look at a feature variables, the inputs, 
and then find the patterns and use them to predict the target variable
- another common naming setting is to use x (data) to predict y (labels)
Training a model takes place on the training data split (model learns the course material)
Goal: Minimise time between experiments

Things to remember!
- some models work better than others on different problems
- don't be afraid to try things (ML is highly iterative process)
- start small and build up (add complexity) as you need

Modelling - Tuning
Model can be tuned with different types of data, usually tuning will take place on a validation data split
Depending on what kind of model you are using will depend on what kind of hyper parameters you can choose
For example, a Random Forest will allow you to adjust the number of trees
and a Neural Network will allow you to adjust the number of layers
- examples of different hyper parameters you can adjust on different kinds of algorithms.

Things to remember!
- machine learning models have hyperparameters you can adjust
(however, depending on what model you are going to use, the hyperparameters will be different)
A goal of tuning hyperparameters is to improve your model's performance
- a models first results are not its last
- tuning can take place on training or validation data sets

Modelling - Comparison
How will our model perform in the real world?

After you have tuned and improved your model's performance through hyperparameter tuning, 
it is time to see how it performs on the test set 
(test set is like the final exam for machine learning models).
If you have created your data splits correctly, 
it should give you an indication on how your model will perform once deployed in production. 

A good model will yield similar results on the training, validation and test sets, 
and it is not uncommon to see a slight decline in performance from the model 
on the training and validation set to the test set.

What you should be worried about is 
if the training set performance is dramatically higher than the test set, also known as underfitting, 
and if the test set performance is higher than the training set performance, also known as overfiting.

Overfitting and Underfitting are both examples of a model not being able to generalize well, 
which is what we don't want.
The ideal model shows up in the Goldilocks zone (balanced).
It fits just right, not too well, but not too poorly. Finding this balanced zone is an iterative process. 

There are several reasons why underfitting and overfitting can happen, but the main ones are: 
data leakage and data mismatch.

Data leakage happens when some of your test data leaks into your training data, and 
this often results in overfitting or a model doing better on the test set than on the training data set.

It is like if you were to have a look at the final exam or everyone had to look at the final exam 
as the practice exam, your machine learning model has just learned what it is about to be test on.
The test data set is used as an indication of how well your model will generalize in the real world 
so you want to avoid data leakage. 

Data mismatch happens when the data you are testing on is different to the data you are training on,
such as having different features in the training data to the test data.
Having this kind of mismatch can lead to models performing poorly on test data compared to training
data (underfitting).

This is why it is important to ensure that training is done on the same kind of data as 
you will be testing on and as close as possible to what you will be using in your future applications.
